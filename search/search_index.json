{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"Application/","title":"Data Ingestion in a Flask Web Application","text":""},{"location":"Application/#introduction","title":"Introduction","text":"<p>This document discusses the data ingestion process within a Flask web application for making predictions using a trained machine learning model. The application takes user inputs, preprocesses the data, and provides predictions. The relevant components are as follows:</p> <ul> <li> <p>Flask: A web framework for building web applications in Python.</p> </li> <li> <p>CustomData: A class representing user input data for predictions.</p> </li> <li> <p>PredictPipeline: A class responsible for loading the trained model, loading a preprocessing object, and making predictions.</p> </li> </ul>"},{"location":"Application/#data-ingestion-steps","title":"Data Ingestion Steps","text":""},{"location":"Application/#1-user-input","title":"1. User Input","text":"<ul> <li> <p>The Flask application has a form that allows users to input various features required for making predictions, such as meal type, lead time, and more.</p> </li> <li> <p>When the user submits the form, the entered data is collected as HTTP POST requests.</p> </li> </ul>"},{"location":"Application/#2-data-creation","title":"2. Data Creation","text":"<ul> <li>The user input data is collected as key-value pairs and used to create a <code>CustomData</code> object, encapsulating the input data.</li> </ul>"},{"location":"Application/#3-data-transformation","title":"3. Data Transformation","text":"<ul> <li>The <code>CustomData</code> object is used to create a DataFrame that matches the structure used during model training. This DataFrame includes the user-provided input data.</li> </ul>"},{"location":"Application/#4-prediction","title":"4. Prediction","text":"<ul> <li> <p>The Flask application utilizes the <code>PredictPipeline</code> to make predictions based on the user's input data.</p> </li> <li> <p>The <code>PredictPipeline</code> loads the trained model and a preprocessing object.</p> </li> <li> <p>The user's input data is preprocessed using the loaded preprocessing object.</p> </li> <li> <p>The preprocessed data is then passed through the trained model, which returns predictions.</p> </li> <li> <p>The prediction result is a label indicating whether the booking will be canceled or not, and it is displayed to the user.</p> </li> </ul>"},{"location":"Application/#usage","title":"Usage","text":"<p>To use the Flask web application for making predictions, follow these steps:</p> <ol> <li> <p>Start the Flask application by running the script.</p> </li> <li> <p>Access the home page by visiting the application's URL.</p> </li> <li> <p>Fill out the form with the required input features.</p> </li> <li> <p>Submit the form to get a prediction result.</p> </li> <li> <p>The result will be displayed on the web page, indicating whether the booking is expected to be canceled or not.</p> </li> </ol> <p></p>"},{"location":"Application/#conclusion","title":"Conclusion","text":"<p>The data ingestion process in the Flask web application involves collecting user input, transforming it into the required format, and using a trained machine learning model to make predictions. This document provides an overview of the data ingestion steps within the context of the web application.</p> <p>This document offers insights into the data ingestion process within a Flask web application, facilitating a better understanding of how user input is collected and processed for making predictions.</p>"},{"location":"Data-Ingestion/","title":"Data Ingestion Process","text":""},{"location":"Data-Ingestion/#introduction","title":"Introduction","text":"<p>This document provides an overview of the data ingestion process implemented in the code, which involves reading data from an Excel file, splitting it into training and testing sets, and saving the data in CSV format.</p>"},{"location":"Data-Ingestion/#code-structure","title":"Code Structure","text":"<p>The data ingestion process is encapsulated in the <code>DataIngestion</code> class and is defined by the following components:</p> <ul> <li> <p>DataIngestionConfig: A data class that stores the file paths for the raw data, training data, and testing data.</p> </li> <li> <p>initiate_data_ingestion(): A method that performs the data ingestion process.</p> </li> </ul>"},{"location":"Data-Ingestion/#data-ingestion-steps","title":"Data Ingestion Steps","text":""},{"location":"Data-Ingestion/#1-reading-data","title":"1. Reading Data","text":"<p>The data ingestion process begins by reading an Excel file named \"bookings.xlsx.\" This dataset serves as the source data for subsequent processing. The data is loaded into a Pandas DataFrame.</p>"},{"location":"Data-Ingestion/#2-creating-directories","title":"2. Creating Directories","text":"<p>Before saving the data, the code creates directories to store the CSV files if they do not already exist. These directories are specified in the <code>DataIngestionConfig</code>.</p>"},{"location":"Data-Ingestion/#3-saving-raw-data","title":"3. Saving Raw Data","text":"<p>The raw data is saved to a CSV file specified by <code>raw_data_path</code>. This file contains the entire dataset as it was read from the Excel file, preserving its integrity.</p>"},{"location":"Data-Ingestion/#4-train-test-split","title":"4. Train-Test Split","text":"<p>A standard train-test split is performed on the dataset using the <code>train_test_split</code> function from the <code>sklearn.model_selection</code> library. The training set contains 80% of the data, while the testing set contains 20%. A random seed (42) is used for reproducibility.</p>"},{"location":"Data-Ingestion/#5-saving-training-and-testing-data","title":"5. Saving Training and Testing Data","text":"<p>Both the training and testing datasets are saved to separate CSV files specified by <code>train_data_path</code> and <code>test_data_path</code>, respectively. These CSV files are ready for use in machine learning models or other data processing tasks.</p>"},{"location":"Data-Ingestion/#exception-handling","title":"Exception Handling","text":"<p>The code includes exception handling to capture and raise any exceptions that may occur during the data ingestion process. These exceptions are caught and converted into a custom exception (<code>CustomException</code>) along with relevant information such as the error message and system details.</p>"},{"location":"Data-Ingestion/#logging","title":"Logging","text":"<p>Logging statements are included in the code to provide insights into the execution of the data ingestion process. Key events and stages in the process are logged to assist in debugging and monitoring.</p>"},{"location":"Data-Ingestion/#usage","title":"Usage","text":"<p>To use the data ingestion process, follow these steps:</p> <ol> <li>Create an instance of the <code>DataIngestion</code> class.</li> <li>Call the <code>initiate_data_ingestion()</code> method on the instance to perform data ingestion.</li> <li>The method returns the file paths for the training and testing data CSV files, which can be used for further analysis and modeling.</li> </ol>"},{"location":"Data-Ingestion/#conclusion","title":"Conclusion","text":"<p>The data ingestion process detailed in this document is a fundamental step in preparing data for machine learning and analysis. It provides clean, structured data in CSV format that can be readily used in various data science projects.</p>"},{"location":"Data-Transformation/","title":"Data Transformation Process","text":""},{"location":"Data-Transformation/#introduction","title":"Introduction","text":"<p>This document outlines the data transformation process implemented in the code, which involves various operations such as feature extraction, outlier removal, feature selection, feature encoding, and scaling. The primary objective is to prepare the data for machine learning tasks.</p>"},{"location":"Data-Transformation/#code-structure","title":"Code Structure","text":"<p>The data transformation process is encapsulated in the <code>DataTransformation</code> class and is defined by the following components:</p> <ul> <li> <p>DataTransformationConfig: A data class that stores the file path for the preprocessor object.</p> </li> <li> <p>Methods for feature extraction, outlier removal, feature selection, feature encoding, oversampling, and standardization.</p> </li> </ul>"},{"location":"Data-Transformation/#data-transformation-steps","title":"Data Transformation Steps","text":""},{"location":"Data-Transformation/#1-feature-extraction","title":"1. Feature Extraction","text":"<p>The <code>feature_extraction</code> method creates a new feature, <code>total_stays</code>, by adding the values of <code>stays_in_weekend_nights</code> and <code>stays_in_week_nights</code>. This new feature can provide additional insights for analysis.</p>"},{"location":"Data-Transformation/#2-outlier-removal","title":"2. Outlier Removal","text":"<p>The <code>outlier_removal</code> method removes outliers from the dataset. Outliers are detected and filtered based on selected numerical columns. The Interquartile Range (IQR) method is used for outlier detection.</p>"},{"location":"Data-Transformation/#3-feature-selection","title":"3. Feature Selection","text":"<p>The <code>feature_selection</code> method removes non-essential features from the dataset. Several columns, including demographic and date-related information, are dropped to focus on relevant features.</p>"},{"location":"Data-Transformation/#4-feature-encoding","title":"4. Feature Encoding","text":"<p>The <code>features_encoding</code> method encodes categorical columns using Target Encoding. Target Encoding is applied to both training and testing datasets separately, and encoded data is obtained for further processing.</p>"},{"location":"Data-Transformation/#5-oversampling-smote","title":"5. Oversampling (SMOTE)","text":"<p>The <code>oversampling_smote</code> method applies the Synthetic Minority Over-sampling Technique (SMOTE) to address imbalanced data in the training dataset. SMOTE generates synthetic samples to balance the class distribution.</p>"},{"location":"Data-Transformation/#6-standardization","title":"6. Standardization","text":"<p>The <code>standardization</code> method performs feature scaling on the data using StandardScaler. This ensures that all features have a mean of 0 and a standard deviation of 1.</p>"},{"location":"Data-Transformation/#7-preprocessing-object","title":"7. Preprocessing Object","text":"<p>The <code>get_data_transformation_object</code> method builds a preprocessing object, which includes encoding and scaling pipelines for both numerical and categorical features.</p>"},{"location":"Data-Transformation/#8-data-transformation","title":"8. Data Transformation","text":"<p>The <code>initiate_data_transformation</code> method orchestrates the entire data transformation process. It involves reading the training and testing data, applying the above steps, and saving the preprocessing object.</p>"},{"location":"Data-Transformation/#usage","title":"Usage","text":"<p>To use the data transformation process, follow these steps:</p> <ol> <li>Create an instance of the <code>DataTransformation</code> class.</li> <li>Call the <code>initiate_data_transformation(train_path, test_path)</code> method on the instance, providing the file paths to the training and testing data CSV files.</li> <li>The method returns preprocessed data arrays and saves the preprocessing object to the specified file path.</li> </ol>"},{"location":"Data-Transformation/#conclusion","title":"Conclusion","text":"<p>The data transformation process described in this document is a critical step in preparing data for machine learning and analysis. It involves feature engineering, data cleaning, and encoding to ensure the data is suitable for training machine learning models.</p>"},{"location":"Dataset/","title":"Home","text":""},{"location":"Dataset/#hotel-booking-dataset","title":"Hotel Booking Dataset","text":"<p>The online hotel reservation channels have dramatically changed booking possibilities and customers\u2019 behavior. A significant number of hotel reservations are called-off due to cancellations or no-shows. The typical reasons for cancellations include change of plans, scheduling conflicts, etc. This is often made easier by the option to do so free of charge or preferably at a low cost which is beneficial to hotel guests but it is a less desirable and possibly revenue-diminishing factor for hotels to deal with.</p> <p>Data Description:</p> <ul> <li>Dataset Name: Hotel Booking Dataset</li> <li>Number of Records: 119,390</li> <li>Time Period: July 1, 2015, to August 31, 2017</li> <li>Data Source: https://www.kaggle.com/datasets/mojtaba142/hotel-booking</li> </ul> <p>Attributes:</p> <ul> <li>Booking_ID: Unique booking ID</li> <li>adults: Number of adults</li> <li>children: Number of children</li> <li>babies: Number of babies</li> <li>stays_in_weekend_nights: Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel</li> <li>stays_in_week_nights: Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel</li> <li>meal: Type of meal booked. Categories are presented in standard hospitality meal packages:           Undefined/SC \u2013 no meal package;         BB \u2013 Bed &amp; Breakfast;         HB \u2013 Half board (breakfast and one other meal \u2013 usually dinner);         FB \u2013 Full board (breakfast, lunch and dinner)</li> <li>required_car_parking_spaces: Number of car parking spaces required by the customer</li> <li>assigned_room_type: Code for the type of room assigned to the booking. Sometimes the assigned room type differs from the      reserved room type due to hotel operation reasons (e.g. overbooking) or by customer request. Code is presented instead of designation for anonymity reasons</li> <li>lead_time: Number of days that elapsed between the entering date of the booking into the PMS and the arrival date</li> <li>arrival_date_year: Year of arrival date</li> <li>arrival_date_month: Month of arrival date with 12 categories: \u201cJanuary\u201d to \u201cDecember\u201d</li> <li>arrival_date_week_number: Week number of the arrival date</li> <li>arrival_date_day_of_month: Day of the month of the arrival date</li> <li>market_segment: Market segment designation. In categories, the term \u201cTA\u201d means \u201cTravel Agents\u201d and \u201cTO\u201d means \u201cTour Operators\u201d</li> <li>reserved_room_type: Code of room type reserved. Code is presented instead of designation for anonymity reasons</li> <li>is_repeated_guest: Value indicating if the booking name was from a repeated guest (1) or not (0)</li> <li>previous_cancellations: Number of previous bookings that were cancelled by the customer prior to the current booking</li> <li>previous_bookings_not_canceled: Number of previous bookings not cancelled by the customer prior to the current booking</li> <li>total_of_special_requests: Number of special requests made by the customer (e.g. twin bed or high floor)</li> <li>average_price_rooms: average_price_rooms</li> <li>is_canceled: Value indicating if the booking was canceled (1) or not (0)</li> </ul> <p>Use Case:</p> <p>This dataset can be used for various purposes, including but not limited to:</p> <ul> <li>Analyzing booking trends over time.</li> <li>Predicting hotel booking cancellations.</li> <li>Understanding customer behavior and preferences.</li> </ul> <p>Data Preview:</p> <p> </p>"},{"location":"Model-Trainer/","title":"Model Training Process","text":""},{"location":"Model-Trainer/#introduction","title":"Introduction","text":"<p>This document outlines the model training process implemented in the code, which involves training multiple machine learning models using the provided training data and selecting the best-performing model. The selected model is then saved for future use in predictions.</p>"},{"location":"Model-Trainer/#code-structure","title":"Code Structure","text":"<p>The model training process is encapsulated in the <code>ModelTrainer</code> class and is defined by the following components:</p> <ul> <li> <p>ModelTrainerConfig: A data class that stores the file path for the trained model.</p> </li> <li> <p>Methods for initiating model training, evaluating different machine learning models, selecting the best model, and saving the best model.</p> </li> </ul>"},{"location":"Model-Trainer/#model-training-steps","title":"Model Training Steps","text":""},{"location":"Model-Trainer/#1-data-splitting","title":"1. Data Splitting","text":"<p>The code starts by splitting the training data into input features (X_train) and the target variable (y_train), as well as the testing data into input features (X_test) and the target variable (y_test).</p>"},{"location":"Model-Trainer/#2-model-selection","title":"2. Model Selection","text":"<p>Several machine learning models are considered for training, including: - Random Forest - Decision Tree - Logistic Regression - XGBoost Classifier</p> <p>Hyperparameters for each model are specified in the <code>params</code> dictionary, allowing for hyperparameter tuning during training.</p>"},{"location":"Model-Trainer/#3-model-evaluation","title":"3. Model Evaluation","text":"<p>The <code>evaluate_models</code> function is called to evaluate each model using the training data. The evaluation includes cross-validation, hyperparameter tuning, and model performance assessment using various metrics.</p>"},{"location":"Model-Trainer/#4-best-model-selection","title":"4. Best Model Selection","text":"<p>The best-performing model is determined based on the evaluation results. The model with the highest score is selected as the best model for further use.</p>"},{"location":"Model-Trainer/#5-model-saving","title":"5. Model Saving","text":"<p>The selected best model is saved to a file, specified in <code>ModelTrainerConfig</code>. This trained model can be loaded for making predictions on new data.</p>"},{"location":"Model-Trainer/#6-model-performance-metrics","title":"6. Model Performance Metrics","text":"<p>Performance metrics such as accuracy, precision, and recall are computed for the selected best model on the test data. These metrics provide insights into the model's effectiveness.</p>"},{"location":"Model-Trainer/#usage","title":"Usage","text":"<p>To use the model training process, follow these steps:</p> <ol> <li>Create an instance of the <code>ModelTrainer</code> class.</li> <li>Call the <code>initiate_model_trainer(train_array, test_array)</code> method on the instance, providing the training and testing data arrays.</li> <li>The method returns the best model, along with accuracy, precision, and recall scores on the test data.</li> </ol>"},{"location":"Model-Trainer/#conclusion","title":"Conclusion","text":"<p>The model training process described in this document is crucial for building machine learning models and selecting the best-performing model for the given dataset. It involves model evaluation, hyperparameter tuning, and performance assessment, resulting in the selection of the best model for future predictions.</p> <p>This document provides a comprehensive overview of the code's model training process, making it easier for users and developers to understand and utilize the best model for predictive tasks.</p>"},{"location":"Prediction-Pipeline/","title":"Predict Pipeline","text":""},{"location":"Prediction-Pipeline/#introduction","title":"Introduction","text":"<p>This document focuses on the data ingestion process as part of the predict pipeline, specifically for making predictions using a trained machine learning model. Data ingestion is crucial for preparing the input data and making predictions. This process is implemented in the <code>PredictPipeline</code> class.</p>"},{"location":"Prediction-Pipeline/#code-structure","title":"Code Structure","text":"<p>The data ingestion process is part of the predict pipeline, which includes model loading and preprocessing. The relevant components are as follows:</p> <ul> <li> <p>PredictPipeline: A class responsible for loading a trained model and preprocessing data for making predictions.</p> </li> <li> <p>CustomData: A class that represents input data for making predictions.</p> </li> </ul>"},{"location":"Prediction-Pipeline/#data-ingestion-steps","title":"Data Ingestion Steps","text":""},{"location":"Prediction-Pipeline/#1-model-and-preprocessor-loading","title":"1. Model and Preprocessor Loading","text":"<p>The data ingestion process starts with the <code>PredictPipeline</code>. It performs the following steps:</p> <ul> <li> <p>Loading Model: The code loads the trained machine learning model from a specified file path, typically saved during the model training process.</p> </li> <li> <p>Loading Preprocessor: The code also loads a preprocessing object, which was saved during data transformation. This object is used to preprocess the input data before making predictions.</p> </li> </ul>"},{"location":"Prediction-Pipeline/#2-data-preprocessing","title":"2. Data Preprocessing","text":"<p>Once the model and preprocessing object are loaded, the data preprocessing process begins. This involves:</p> <ul> <li> <p>Input Data Creation: The <code>CustomData</code> class is used to create input data for making predictions. It encapsulates various input features required for predictions, such as meal type, room type, lead time, and more.</p> </li> <li> <p>Data Transformation: The input data is transformed into a DataFrame, making it compatible with the preprocessing object. The DataFrame has the same structure as the data used during model training.</p> </li> <li> <p>Preprocessing: The loaded preprocessing object is applied to the input data, performing operations like feature encoding and scaling.</p> </li> </ul>"},{"location":"Prediction-Pipeline/#3-prediction","title":"3. Prediction","text":"<p>With preprocessed data, the code uses the loaded model to make predictions. The output is a prediction or a classification result based on the input data.</p>"},{"location":"Prediction-Pipeline/#usage","title":"Usage","text":"<p>To use the predict pipeline for making predictions, follow these steps:</p> <ol> <li> <p>Create an instance of the <code>PredictPipeline</code> class.</p> </li> <li> <p>Call the <code>predict(features)</code> method on the instance, providing the input data as a dictionary of feature values. The features must match the structure used during model training.</p> </li> <li> <p>The method performs data ingestion and preprocessing, and returns predictions based on the loaded model.</p> </li> </ol>"},{"location":"Prediction-Pipeline/#conclusion","title":"Conclusion","text":"<p>The data ingestion process in the predict pipeline is a crucial step in preparing and preprocessing data for making predictions with a trained machine learning model. This document provides an overview of the data ingestion process within the context of the predict pipeline.</p> <p>This document offers insights into the data ingestion process within the predict pipeline, facilitating a better understanding of how input data is prepared and preprocessed before making predictions.</p>"},{"location":"Training-Pipeline/","title":"Training Pipeline","text":""},{"location":"Training-Pipeline/#introduction","title":"Introduction","text":"<p>This document focuses on both data ingestion and data transformation processes as part of the training pipeline. These processes are vital for gathering, preparing, and transforming the training data for subsequent machine learning model training. These processes are implemented in the <code>TrainingPipeline</code> class.</p>"},{"location":"Training-Pipeline/#code-structure","title":"Code Structure","text":"<p>The data ingestion and transformation processes are part of a larger training pipeline that includes model training and experiment tracking. The relevant components are as follows:</p> <ul> <li> <p>TrainingPipeline: A class that orchestrates the entire training process, including data ingestion, transformation, model training, and experiment tracking.</p> </li> <li> <p>DataIngestion: A component responsible for loading and preparing the training and testing data.</p> </li> <li> <p>DataTransformation: A component responsible for feature extraction, outlier removal, feature selection, feature encoding, and data scaling.</p> </li> </ul>"},{"location":"Training-Pipeline/#data-ingestion-steps","title":"Data Ingestion Steps","text":""},{"location":"Training-Pipeline/#1-data-ingestion","title":"1. Data Ingestion","text":"<p>The data ingestion process starts with the <code>DataIngestion</code> component. It performs the following steps:</p> <ul> <li> <p>Data Reading: The code reads data from a source (in this case, \"notebook/bookings.xlsx\") using the Pandas library, creating a DataFrame.</p> </li> <li> <p>Directory Creation: It creates directories to store CSV files for raw data, training data, and testing data.</p> </li> <li> <p>Data Splitting: The data is split into training and testing sets using an 80/20 split ratio, and both sets are saved as CSV files in their respective directories.</p> </li> </ul>"},{"location":"Training-Pipeline/#data-transformation-steps","title":"Data Transformation Steps","text":""},{"location":"Training-Pipeline/#2-data-transformation","title":"2. Data Transformation","text":"<p>The data transformation process is handled by the <code>DataTransformation</code> component and involves the following steps:</p> <ul> <li> <p>Feature Extraction: The component adds a new feature, <code>total_stays</code>, by combining the values of <code>stays_in_weekend_nights</code> and <code>stays_in_week_nights</code>.</p> </li> <li> <p>Outlier Removal: Outliers are removed from the dataset based on specific numerical columns using the Interquartile Range (IQR) method.</p> </li> <li> <p>Feature Selection: Non-essential features, such as demographic and date-related information, are removed from the dataset to focus on relevant features.</p> </li> <li> <p>Feature Encoding: Categorical columns are encoded using Target Encoding, separately for training and testing datasets.</p> </li> <li> <p>Oversampling (SMOTE): The Synthetic Minority Over-sampling Technique (SMOTE) is applied to address imbalanced data in the training dataset.</p> </li> <li> <p>Standardization: Feature scaling is performed using StandardScaler to ensure that all features have a mean of 0 and a standard deviation of 1.</p> </li> </ul>"},{"location":"Training-Pipeline/#3-training-pipeline","title":"3. Training Pipeline","text":"<p>The <code>TrainingPipeline</code> class is responsible for orchestrating the entire training process. It includes the following steps:</p> <ul> <li> <p>Model Training: The pipeline initiates model training using the data prepared by the <code>DataIngestion</code> and <code>DataTransformation</code> components.</p> </li> <li> <p>Model Evaluation: It evaluates the trained models, selecting the best-performing model based on various metrics.</p> </li> <li> <p>Experiment Creation: The pipeline creates an experiment using MLflow, logs the model, and associated metrics.</p> </li> <li> <p>Remote Server Configuration (DAGShub): Optionally, it can be configured to work with remote servers like DAGShub for model tracking and registry.</p> </li> </ul>"},{"location":"Training-Pipeline/#usage","title":"Usage","text":"<p>To use the training pipeline for model training and experiment tracking, follow these steps:</p> <ol> <li> <p>Create an instance of the <code>TrainingPipeline</code> class.</p> </li> <li> <p>Call the <code>start_model_training</code> method on the instance to initiate the data ingestion and transformation, as well as model training and experiment tracking.</p> </li> <li> <p>Specify the experiment name, run name, accuracy, precision, recall, best model, confusion matrix path, ROC AUC plot path, and run parameters if needed.</p> </li> <li> <p>The pipeline orchestrates the process, trains the model, and logs relevant information to the MLflow tracking server or a remote server like DAGShub.</p> </li> </ol>"},{"location":"Training-Pipeline/#conclusion","title":"Conclusion","text":"<p>The data ingestion and transformation processes are crucial initial steps in the training pipeline. They ensure that the training and testing data are correctly loaded, prepared, and transformed for subsequent model training and experiments. This document provides an overview of both processes within the larger training pipeline.</p> <p>This document offers insights into the data ingestion and transformation processes within the broader context of the training pipeline, facilitating a better understanding of how data is prepared and transformed for model training and experiments.</p>"}]}